End to end ML Project implementation

DAY 1 
* Agenda
1. Intriduction
2. Github repo setup
3. Project template creation
4. Project setup
5. Logging, exception and utility
6. Project workfilows
7. Notebook experiments
8. Components implementation
	1. Data ingestion
	2. Data validation
	3. Data transformation
	4. Model training
	5. Model evaluation
9. Training pipeline
10. Prediction pipeline
11. User app implementation
12. Dockerization
13. Deployment on AWS - CICD

* Prerequisite :
1. Python progrsmming - OOP's
2. Machine Learning concept
3. AWS account - Deployment
4. System configuration - 
5. Your dedication

* Tech stack
1. pandas
2. sk-learn
3. matplotlib / seaborn
4. pyyaml 
5. Flask/ UI app
6. AWS - Deployment

* End to end implementation :
1. Project structure should not be always fixed.
2. Why OOPs concepty is required for end to end.

* Today :
1. Project introduction
2. Neurolab setup
3. Github setup with neurolab
4. Project template
5. Requirement installation and project setup

===================================================

1. Create repo on github
2. Clone repo : git clone https://github.com/mungekarkiran/End-to-end-ML-project.git
3. Move to folder
4. Create a file : touch template.py
5. Push file to github :
> git add .
> git commit -m "first commit"
	- git config --global user.email "mungekarkiran05@gmail.com"
	- git config --global user.name "mungekarkiran" 
> git push origin main
> git pull origin main
6. Copy code in to template.py
7. Run the template.py file
	- Cookiecutter is a templating library for creating project boilerplates in any programming language.
8. Push the changes on github
9. For project setup create virtual env.
> conda create -n ml_proj python=3.8 -y
> conda activate ml_proj
9.1. Copy code in setup.py file
	- REPO_NAME = "End-to-end-ML-project"
	- AUTHOR_USER_NAME = "mungekarkiran"
	- SRC_REPO = "red_wine_quality_project"
	- AUTHOR_EMAIL = "mungekarkiran05@gmail.com"
 > pip install -r requirements.txt
10. Push the changes on github

===================================================

Day 2
* Today :
1. Notebook experiment
2. Project utility : - logging, exceptions (Box packaging - Box exception), utils module
3. Project workflow


Wine Quality data

load data -> EDA ->  

1. Load data to research folder
2. Create Experiment.ipynb file in research folder
3. Select env in Experiment.ipynb file (ml_proj python3.8).
4. Notebook experiment
5. Commite the code

6. Add logging functionality (We have 2 ways)
- src/red_wine_quality_project/logging
	- src/red_wine_quality_project/logging/__init__.py
	- write logging code
	- import logging in main.py
	- run main.py

- src/red_wine_quality_project/__init__.py # constructor file
	- copy and pest the src/red_wine_quality_project/logging/__init__.py code
	- Commite the code

7. create utility module
-  src/red_wine_quality_project/utils/common.py write utility related functions
- ex. : config.yaml and schema.yaml, to read yaml file we have to write a function, the function using frequently in your code; instant of creating that in in each component you just create a utility/common.py. like def read_yaml(), write_yaml(), load_object(), save_object(), etc.

- End-to-end-ML-project/research/trials.ipynb

8. exceptions (Box packaging - Box exception)
- from box.exceptions import BoxValueError

9. Project workfilows

	1. update config.yaml =>  
	2. update schema.yaml => while creating training part that time we update this file.
	3. update params.yaml => this is required when we initialize Model.
	4. update the entity => 
	5. update the configuration manager in src config => writing all configuration manager.
	6. update the components => we create our components like data ingestion, ..., etc.
	7. update the pipeline => training pipeline and prediction pipeline.
	8. update the main.py => 
	9. update the app.py => part which used to combine with flask

10. commit the changes

===================================================

Day 3
* Today :
1. Data ingestion component -> Github
2. Dava validation component -> Validate columns
3. Data transformation component -> Train and val data

- on Jupyter Notebook and then on VS code

* Data ingestion component
1. create End-to-end-ML-project\research\01_Data_ingestion.ipynb file
2. update config.yaml => 
- create folder artifacts
- download file from github like : https://github.com/entbappy/Branching-tutorial/raw/master/winequality-data.zip -> artifacts/data_ingestion/data.zip
- unzip that file artifacts/data_ingestion/data.csv

3. update schema.yaml
- End-to-end-ML-project\schema.yaml
- update schema as per csv columns

4. update params.yaml
- End-to-end-ML-project\params.yaml
- initialy -> key: value

5. update the entity
- in research\01_Data_ingestion.ipynb file first

6. update src\red_wine_quality_project\constants
- adde config, param and schema Path
- in research\01_Data_ingestion.ipynb file first

7. update the configuration manager in src config
- in research\01_Data_ingestion.ipynb file first :: class ConfigurationManager 

8. update the components
- in research\01_Data_ingestion.ipynb file first :: class DataIngestion

9. update the pipeline 
- in research\01_Data_ingestion.ipynb file first :: class DataIngestion

10. move to .py file 
- update the entity : End-to-end-ML-project/src/red_wine_quality_project/entity/config_entity.py
- update the configuration manager in src config : End-to-end-ML-project/src/red_wine_quality_project/config/configuration.py
- update the components : End-to-end-ML-project/src/red_wine_quality_project/components/data_ingestion.py
- update the pipeline : End-to-end-ML-project/src/red_wine_quality_project/pipeline/stage_01_data_ingestion.py
- update the main.py : End-to-end-ML-project/main.py

* Dava validation component
1. add configuration : End-to-end-ML-project/config/config.yaml
2. update entity in notebook : End-to-end-ML-project/research/02_Data_validation.ipynb
3. update the configuration manager in src config : End-to-end-ML-project/research/02_Data_validation.ipynb
4. update the components : End-to-end-ML-project/research/02_Data_validation.ipynb
5. update the pipeline : End-to-end-ML-project/research/02_Data_validation.ipynb
6. do the same with .py files from 2 to 5.

7. commit the changes.

* Data transformation component
1. add configuration : End-to-end-ML-project/research/03_Data_transformation.ipynb
2. update entity in notebook : End-to-end-ML-project/research/03_Data_transformation.ipynb
3. update the configuration manager in src config : End-to-end-ML-project/research/03_Data_transformation.ipynb
4. update the components : End-to-end-ML-project/research/03_Data_transformation.ipynb
5. update the pipeline : End-to-end-ML-project/research/03_Data_transformation.ipynb
6. do the same with .py files from 2 to 5.

7. commit the changes.

===================================================

Day 4
* Today :
1. Model training component
2. Model evaluation component
3. Prediction pipeline
4. Web app -> flask

* Model training component
1. add configuration : End-to-end-ML-project/research/04_Model_trainer.ipynb
	1.1. update params.yaml file : End-to-end-ML-project/params.yaml
2. update entity in notebook : End-to-end-ML-project/research/04_Model_trainer.ipynb
3. update the configuration manager in src config : End-to-end-ML-project/research/04_Model_trainer.ipynb
4. update the components : End-to-end-ML-project/research/04_Model_trainer.ipynb
5. update the pipeline : End-to-end-ML-project/research/04_Model_trainer.ipynb
6. do the same with .py files from 2 to 5.

7. commit the changes.


* Model evaluation component
1. add configuration : End-to-end-ML-project/research/05_Model_evaluation.ipynb
2. update entity in notebook : End-to-end-ML-project/research/05_Model_evaluation.ipynb
3. update the configuration manager in src config : End-to-end-ML-project/research/05_Model_evaluation.ipynb
4. update the components : End-to-end-ML-project/research/05_Model_evaluation.ipynb
5. update the pipeline : End-to-end-ML-project/research/05_Model_evaluation.ipynb
6. do the same with .py files from 2 to 5.

7. commit the changes.



